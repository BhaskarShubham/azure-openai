{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3290ad6d-c2f7-4e4f-93aa-1c4db8711967",
   "metadata": {},
   "source": [
    "# Azure OpenAI Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b90211-5fce-486c-9509-f50293573d9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install openai"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0d6eb03a-2904-4532-b3ae-950159c4c30c",
   "metadata": {},
   "source": [
    "# Integrate Azure OpenAI Into your applications\n",
    "-  You have deployed a model in Azure OpenAI Service\n",
    "- You have the endpoint address\n",
    "- Deployment Name\n",
    "- azure open api key: Export to enviornment variable\n",
    "- api verson\n",
    "- Have installed openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1aff3f-c3ff-4182-a4bd-0849f225b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import AzureOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38457430-ddc8-47fd-8b66-7eba459fd811",
   "metadata": {},
   "source": [
    "## Setup client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b27bdf2-5d4b-493c-a520-1664e7f86a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = os.getenv(\"f71bf174b86e4296b780492e1d8c6678\")\n",
    "version = \"2023-07-01-preview\"\n",
    "endpoint = \"https://openai1508.openai.azure.com/\"\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  api_key = key,  \n",
    "  api_version = version,\n",
    "  azure_endpoint = endpoint\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b520a17f-cda7-419c-83aa-66d3102978fe",
   "metadata": {},
   "source": [
    "## ChatCompletions vs Completions\n",
    "ChatCompletion in Azure OpenAI models offers more flexibility and optimization for chat scenarios compared to Completion. It allows defining a system message and built-in structure for previous messages in the prompt. While both endpoints can use similar models, only ChatCompletion is compatible with gpt-4 generation models. The Completion endpoint can achieve similar results with clear prompt formatting, but ChatCompletion is more versatile, even for non-chat scenarios with instructions in the system message and user content in the user role message.\n",
    "\n",
    "## Adjusting model parameters\n",
    "Adjusting model parameters, specifically temperature and top_p, significantly influences responses. Higher values increase creativity and randomness but reduce consistency. Lower values provide more focused and consistent results. Experiment with these parameters individually for optimal outcomes, avoiding simultaneous changes to both temperature and top_p."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfe88f7-d698-4a88-a212-3d15b8fd88fc",
   "metadata": {},
   "source": [
    "### Send Chat Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15c5049-2ca3-41b4-b04d-50e6b8cffc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_text=[\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Who were the founders of Microsoft?\"}\n",
    "    ]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Chat\",\n",
    "    messages=message_text,\n",
    "    temperature=0.7,\n",
    "    max_tokens=800,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5cc780-ff9c-4af9-8169-eca7c53b4f1b",
   "metadata": {},
   "source": [
    "### Print Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21101683-4e9c-40c9-9d82-639e9f6e46d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0718fd1e-6a90-4eaa-9ecf-92b6e8d8fba9",
   "metadata": {},
   "source": [
    "# Prompt Engineering with Azure OpenAI SDK "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccf51d5-a770-4582-a9c2-e986a0bcb540",
   "metadata": {},
   "source": [
    "## Provide clear instructions\n",
    "Asking the Azure OpenAI model clearly for what you want is one way to get desired results. By being as descriptive as possible, the model can generate a response that most closely matches what you're looking for.\n",
    "\n",
    "EXERCISE: Compare the responsed for the following message texts:\n",
    "\n",
    "message_text=[\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Write a product description for a new water bottle\"}\n",
    "    ]\n",
    "\n",
    "and\n",
    "\n",
    "message_text=[\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "        {\"role\": \"user\", \"Write a product description for a new water bottle that is 100% recycled. Be sure to include that\n",
    "it comes in natural colors with no dyes, and each purchase removes 10 pounds of plastic from our \n",
    "oceans\"}\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7125c87e-1753-48c1-acf5-61ba9a44cb51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bf468047-b306-43cd-8e38-314592f8e9b1",
   "metadata": {},
   "source": [
    "## Format of instructions\n",
    "\n",
    "Formatting and recency: Model interprets prompts based on format and prioritizes ending info. Repeat crucial instructions at the end for better responses. Same applies to chats: recent messages hold more weight. Important info towards the end = better outputs.\n",
    "\n",
    "## Use section markers\n",
    "A specific technique for formatting instructions is to split the instructions at the beginning or end of the prompt, and have the user content contained within --- or ### blocks. These tags allow the model to more clearly differentiate between instructions and content. For example:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff24a06-1eb7-4255-8762-9f40a37fcb44",
   "metadata": {},
   "source": [
    "## Provide context to improve accuracy\n",
    "By providing context to the AI model, it allows the model to better understand what you are asking for or what it should know to provide the best answer. Context can be provided in several ways."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33d14a3-4f6b-49d4-8f45-c858c18e9664",
   "metadata": {},
   "source": [
    "## Request output composition\n",
    "Specifying the structure of your output can have a large impact on your results. This could include something like asking the model to cite their sources, write the response as an email, format the response as a SQL query, classify sentiment into a specific structure, and so on. For example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae90ad2-d7c3-44ae-a73a-456c9fe21f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "content=\"Write a table in markdown with 6 animals in it, with their genus and species\"\n",
    "\n",
    "message_text=[\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Chat\",\n",
    "    messages=message_text,\n",
    "    temperature=0.7,\n",
    "    max_tokens=800,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a2b128-1903-4651-b5fa-0914fe0bd1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0535016f-c901-48db-8581-cf4b2ba99367",
   "metadata": {},
   "source": [
    "## System message\n",
    "The system message is included at the beginning of a prompt and is designed to give the model instructions, perspective to answer from, or other information helpful to guide the model's response. This system message might include tone or personality, topics that shouldn't be included, or specifics (like formatting) of how to answer.\n",
    "\n",
    "Try the following:\n",
    "\n",
    "- \"I want you to act like a command line terminal. Respond to commands exactly as cmd.exe would, in one unique code block, and nothing else.\"\n",
    "- \"I want you to be a translator, from English to Spanish. Don't respond to anything I say or ask, only translate between those two languages and reply with the translated text.\"\n",
    "- \"Act as a motivational speaker, freely giving out encouraging advice about goals and challenges. You should include lots of positive affirmations and suggested activities for reaching the user's end goal.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208a3041-813c-4635-ab73-b5427ef49c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message=\"I want you to be a translator, from English to Spanish. Don't respond to anything I say or ask, only translate between those two languages and reply with the translated text.\"\n",
    "content=\"When will this class be over?\"\n",
    "\n",
    "message_text=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Chat\",\n",
    "    messages=message_text,\n",
    "    temperature=0.7,\n",
    "    max_tokens=800,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088501a3-e576-485d-b98c-113fca93a58f",
   "metadata": {},
   "source": [
    "## Conversation history\n",
    "Along with the system message, other messages can be provided to the model to enhance the conversation. Conversation history enables the model to continue responding in a similar way (such as tone or formatting) and allow the user to reference previous content in subsequent queries. This history can be provided in two ways: from an actual chat history, or from a user defined example conversation.\n",
    "\n",
    "Chat interfaces that use OpenAI models, such as ChatGPT and the chat playground in Azure OpenAI Studio, include conversation history automatically which results in a richer, more meaningful conversation. In the Parameters section below the chat window of the Azure OpenAI Studio chat playground, you can specify how many past messages you want included. Try reducing that to 1 or increasing to max to see how different amounts of history impact the conversation.\n",
    "\n",
    "Note: More conversation history included in the prompt means a larger number of input tokens are used. You will have to determine what the correct balance is for your use case, considering the token limit of the model you are using.\n",
    "\n",
    "Chat systems can also utilize the summarization capabilities of the model to save on input tokens. An app can choose to summarize past messages and include that summary in the conversation history, then provide only the past couple messages verbatim to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4431c37e-2369-4ef4-9c9f-85a42fef0baa",
   "metadata": {},
   "source": [
    "## Few shot learning\n",
    "Using a user defined example conversation is what is called few shot learning, which provides the model examples of how it should respond to a given query. These examples serve to train the model how to respond.\n",
    "\n",
    "For example, by providing the model a couple prompts and the expected response, it continues in the same pattern without instructing it what to do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aa189a-6430-4167-9fb9-3cd29f3eed02",
   "metadata": {},
   "outputs": [],
   "source": [
    "message_text=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"That was an awesome experience\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"positive\"},\n",
    "    {\"role\": \"user\", \"content\": \"I won't do that again\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"negative\"},\n",
    "    {\"role\": \"user\", \"content\": \"That was not worth my time\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"negative\"},\n",
    "    {\"role\": \"user\", \"content\": \"You can't miss this\"}\n",
    "]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Chat\",\n",
    "    messages=message_text,\n",
    "    temperature=0.7,\n",
    "    max_tokens=800,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9df9d45-0bd9-4a96-8d3b-71b87e4f0c18",
   "metadata": {},
   "source": [
    "In practical terms, conversation history and few shot learning are sent to the model in the same way; each user message and assistant response is a discrete message in the message object. The ChatCompletion endpoint is optimized to include message history, regardless of if this message history is provided as few shot learning, or actual conversation history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d7b0b3-c061-4b8c-b5c9-7f5c071a538d",
   "metadata": {},
   "source": [
    "### Break down a complex task\n",
    "Another technique for improved interaction is to divide complex prompts into multiple queries. This allows the model to better understand each individual part, and can improve the overall accuracy. Dividing your prompts also allows you to include the response from a previous prompt in a future prompt, and using that information in addition to the capabilities of the model to generate interesting responses.\n",
    "\n",
    "For example, you could ask the model Doug can ride down the zip line in 30 seconds, and takes 5 minutes to climb back up to the top. How many times can Doug ride the zip line in 17 minutes?. The result is likely 3, which if Doug starts at the top of the zip line is incorrect.\n",
    "\n",
    "A more informative answer could come from asking it multiple questions, about the round trip time to get back to the top of the zip line, and how to account for the fact that Doug starts at the top. Breaking down this problem reveals that Doug can, in fact, ride the zip line four times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628c053c-16f1-4b77-8e21-af15709f5cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "content=\" Doug can ride down the zip line in 30 seconds, and takes 5 minutes to climb back up to the top. How many times can Doug ride the zip line in 17 minutes?\"\n",
    "message_text=[\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Chat\",\n",
    "    messages=message_text,\n",
    "    temperature=0.7,\n",
    "    max_tokens=800,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f9ec35-3271-46fc-9569-abb167d9b889",
   "metadata": {},
   "source": [
    "### Exercise: add in the chat history into the message and break down the task into several message to get the correct answer of 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37528866-c9ff-43d6-a093-d2163c96a895",
   "metadata": {},
   "source": [
    "### Chain of thought\n",
    "One useful method to help you break down your task effectively is to ask the model to explain its chain of thought.\n",
    "\n",
    "Asking a model to respond with the step by step process by which it determined the response is a helpful way to understand how the model is interpreting the prompt. By doing so, you can see where the model made an incorrect logical turn and better understand how to change your prompt to avoid the error.\n",
    "\n",
    "For example, asking the model **What sport is easiest to learn but hardest to master?** results in response with an answer, and a small explanation of why. However, when prompted with **What sport is easiest to learn but hardest to master? Give a step by step approach of your thoughts, ending in your answer** the response is a complete explanation of how it arrived at its answer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2a02f4-d8a3-43f1-9955-07d30a490b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "content=\"What sport is easiest to learn but hardest to master? \"\n",
    "message_text=[\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Chat\",\n",
    "    messages=message_text,\n",
    "    temperature=0.7,\n",
    "    max_tokens=800,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b11d66a-ed91-4ad3-9358-3aff288dc8cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "content=\"What sport is easiest to learn but hardest to master? Give a step by step approach of your thoughts, ending in your answer\"\n",
    "message_text=[\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Chat\",\n",
    "    messages=message_text,\n",
    "    temperature=0.7,\n",
    "    max_tokens=800,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "98e14ee0-a848-4c78-a4f8-72e4b84f2858",
   "metadata": {},
   "source": [
    "# Generating Code with Azure OpenAI SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bbea155-32cf-4563-9aec-84d02e782ecd",
   "metadata": {},
   "source": [
    "The Azure OpenAI service can help developers generate and improve code in various languages for better efficiency and understanding.\n",
    "\n",
    "Newer models like GPT-3.5 and 4 handle both code and language well, eliminating the need for specialized Codex models. **GPT-3.5 Turbo** examples used here. Deploy a GPT-3.5 Turbo model here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd931d7b-2a07-416f-aadc-a0c38cce9c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "content=\"write a function for binary search in python\"\n",
    "message_text=[\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Chat\",\n",
    "    messages=message_text,\n",
    "    temperature=0.7,\n",
    "    max_tokens=800,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452b6500-57cd-4a73-a367-39431bab3394",
   "metadata": {},
   "source": [
    "## Change coding language\n",
    "try this example of changing python code to C#\n",
    "Use triple quoates \"\"\" for multiline statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0703eb58-19e5-4fbe-a0b6-7235a83ad1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "content=\"\"\"\n",
    "convert this code to C#\n",
    "-----------------------\n",
    "def print_squares(n):  \n",
    "    for i in range(1, n+1):  \n",
    "        print(i**2)   \n",
    "\"\"\"\n",
    "message_text=[\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Chat\",\n",
    "    messages=message_text,\n",
    "    temperature=0.7,\n",
    "    max_tokens=800,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79df3454-789b-4647-822f-fcdded123536",
   "metadata": {},
   "source": [
    "## Understand unknown code\n",
    "Azure OpenAI models are helpful for understanding code that doesn't make sense, or may be in a language you aren't familiar with. For example, say you were given the following function (in a fictitious coding language!) and didn't know how to understand it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46a054a-ffdf-4644-9c8c-8462a8c74363",
   "metadata": {},
   "outputs": [],
   "source": [
    "content=\"\"\"\n",
    "Can you explain to me how this code works?:\n",
    "-------------------------\n",
    "fn qzplv(n: i32, m: i32) -> i32 {\n",
    "    if n == 0 {\n",
    "        return m + 1;\n",
    "    } else if m == 0 {\n",
    "        return qzplv(n - 1, 1);\n",
    "    } else {\n",
    "        return qzplv(n - 1, qzplv(n, m - 1));\n",
    "    }\n",
    "}\n",
    "\"\"\"\n",
    "message_text=[\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Chat\",\n",
    "    messages=message_text,\n",
    "    temperature=0.7,\n",
    "    max_tokens=800,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41e51ff-2d81-4238-8aba-866d3e5a11e0",
   "metadata": {},
   "source": [
    "## Complete code and assist the development process\n",
    "Azure OpenAI llms you can speed up your coding workflow by taking care of tedious tasks like:\n",
    "\n",
    "- Writing unit tests: Catch bugs early and guarantee code quality.\n",
    "- Completing partial code: Save time by filling in missing bits.\n",
    "- Commenting code: Improve readability and future maintainability.\n",
    "- Generating documentation: Create clear instructions for others to understand your code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98922b3-9b40-45c1-831d-0c76eb6ba6bc",
   "metadata": {},
   "source": [
    "## Complete partial code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209cc40d-de0d-4b7b-9d73-6d8e08e16896",
   "metadata": {},
   "outputs": [],
   "source": [
    "content=\"\"\"\n",
    "complete the following function  \n",
    "----\n",
    "# calculate the average of the numbers in an array, but only if they're even  \n",
    "def\n",
    "\"\"\"\n",
    "message_text=[\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Chat\",\n",
    "    messages=message_text,\n",
    "    temperature=0.7,\n",
    "    max_tokens=800,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07ef1fb-494a-48b2-a07b-a8b7b24d3d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "content=\"\"\"\n",
    "complete the following code  \n",
    "----- \n",
    "def func1(n)\n",
    "  if n==0:\n",
    "\"\"\"\n",
    "message_text=[\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Chat\",\n",
    "    messages=message_text,\n",
    "    temperature=0.7,\n",
    "    max_tokens=800,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b298c6e7-3b05-47bd-a000-5d3af8f29eb6",
   "metadata": {},
   "source": [
    "## Write unit tests\n",
    "generate unit tests for functions you write to help make your code more robust. Take for example the binary search function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862b54fa-73c3-48a1-9e6b-5641aee76286",
   "metadata": {},
   "outputs": [],
   "source": [
    "content=\"\"\"\n",
    "write three unit tests for this function\n",
    "----- \n",
    "def binary_search(arr, target):\n",
    "    left, right = 0, len(arr) - 1\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        if arr[mid] == target:\n",
    "            return mid\n",
    "        elif arr[mid] < target:\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    return -1\n",
    "\"\"\"\n",
    "message_text=[\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Chat\",\n",
    "    messages=message_text,\n",
    "    temperature=0.7,\n",
    "    max_tokens=800,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5552182-541a-449e-ab8a-b61426ab4e0a",
   "metadata": {},
   "source": [
    "## Add comments and generate documentation\n",
    "AI models can add comments and documentation for you. Take the following function as an example, which can be a little hard to understand when first reading it without any code comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1440ffc3-ffc6-4bec-b03e-0ca83eb2c4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "content=\"\"\"\n",
    "Add comments to this code:\n",
    "----- \n",
    "def permutations(lst):  \n",
    "    if len(lst) == 0:  \n",
    "        return []  \n",
    "    elif len(lst) == 1:  \n",
    "        return [lst]  \n",
    "    else:  \n",
    "        result = []  \n",
    "        for i in range(len(lst)):  \n",
    "            temp = lst[i]  \n",
    "            remaining = lst[:i] + lst[i+1:]  \n",
    "            for p in permutations(remaining):  \n",
    "                result.append([temp] + p)  \n",
    "        return result \n",
    "\"\"\"\n",
    "message_text=[\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Chat\",\n",
    "    messages=message_text,\n",
    "    temperature=0.7,\n",
    "    max_tokens=800,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa5c0f7-e925-4f7a-ba8c-3076eb4d6c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "content=\"\"\"\n",
    "Add comments to this code and documentation for the function:\n",
    "----- \n",
    "def permutations(lst):  \n",
    "    if len(lst) == 0:  \n",
    "        return []  \n",
    "    elif len(lst) == 1:  \n",
    "        return [lst]  \n",
    "    else:  \n",
    "        result = []  \n",
    "        for i in range(len(lst)):  \n",
    "            temp = lst[i]  \n",
    "            remaining = lst[:i] + lst[i+1:]  \n",
    "            for p in permutations(remaining):  \n",
    "                result.append([temp] + p)  \n",
    "        return result \n",
    "\"\"\"\n",
    "message_text=[\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Chat\",\n",
    "    messages=message_text,\n",
    "    temperature=0.7,\n",
    "    max_tokens=800,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fc7b86-5b77-4bc7-9baa-c69f4bd02112",
   "metadata": {},
   "source": [
    "## Fix bugs in your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e58735e-c287-4f1c-871d-a522646a83b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "content=\"\"\"\n",
    "Fix the bugs in this function and explain what was wrong with it:\n",
    "----- \n",
    "def calculate_average(numbers):  \n",
    "    total = 0  \n",
    "    for i in range(len(numbers)):  \n",
    "        number = numbers[i]  \n",
    "        total += number  \n",
    "    average = total  \n",
    "    rerun averave  \n",
    "\"\"\"\n",
    "message_text=[\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Chat\",\n",
    "    messages=message_text,\n",
    "    temperature=0.7,\n",
    "    max_tokens=800,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcd62d8-17a9-40de-a074-b6a7f38694d0",
   "metadata": {},
   "source": [
    "## Improve performance\n",
    "(Win at leetcode?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2908da38-05cb-4a41-980f-32544694aa3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "content=\"\"\"\n",
    "Improve the performance of this code:\n",
    "----- \n",
    "def sum_of_n(n):\n",
    "    result = 0\n",
    "    for i in range(1, n+1):\n",
    "        result += i\n",
    "    return result\n",
    "\"\"\"\n",
    "message_text=[\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Chat\",\n",
    "    messages=message_text,\n",
    "    temperature=0.7,\n",
    "    max_tokens=800,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1a7683-17e9-4a80-bc39-bba5db10a543",
   "metadata": {},
   "source": [
    "## Refactor inefficient code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1226bd3c-7a01-46a3-8c97-9e786bf1a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "content=\"\"\"\n",
    "Can you refactor this code?:\n",
    "----- \n",
    "def calculateTotalPrice(item, quantity):\n",
    "    if item == 'apple':\n",
    "        return quantity * 0.5\n",
    "    elif item == 'banana':\n",
    "        return quantity * 0.75\n",
    "    elif item == 'orange':\n",
    "        return quantity * 0.6\n",
    "    else:\n",
    "        return 0\n",
    "\"\"\"\n",
    "message_text=[\n",
    "        {\"role\": \"system\", \"content\": \"Assistant is a large language model trained by OpenAI.\"},\n",
    "        {\"role\": \"user\", \"content\": content}\n",
    "    ]\n",
    "response = client.chat.completions.create(\n",
    "    model=\"Chat\",\n",
    "    messages=message_text,\n",
    "    temperature=0.7,\n",
    "    max_tokens=800,\n",
    "    top_p=0.95,\n",
    "    frequency_penalty=0,\n",
    "    presence_penalty=0,\n",
    "    stop=None\n",
    ")\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
